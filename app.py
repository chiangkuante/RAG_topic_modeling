import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import os
import sys
from pathlib import Path

# å°‡srcç›®éŒ„æ·»åŠ åˆ°è·¯å¾‘
src_path = Path(__file__).parent / "src"
sys.path.insert(0, str(src_path))

from src.utils.data_processor import DataProcessor
from src.utils.vectorizer import VectorDatabase, TopicVectorizer
from src.utils.langchain_llm_manager import LangChainLLMManager, LLMProvider, LLMConfig
from src.agents.langchain_iteration_controller import LangChainTopicModelingController, LangChainBatchTopicModeling
from src.evaluation.effectiveness_evaluator import EffectivenessEvaluator
from src.evaluation.diversity_evaluator import DiversityEvaluator
from src.evaluation.reliability_evaluator import ReliabilityEvaluator
from src.ui.results_display import display_topic_modeling_results

# é é¢é…ç½®
st.set_page_config(
    page_title="ç–«è‹—ä¸»é¡Œå»ºæ¨¡RAGç³»çµ±",
    page_icon="ğŸ§¬",
    layout="wide",
    initial_sidebar_state="expanded"
)

# åˆå§‹åŒ–session state
if 'data_processor' not in st.session_state:
    st.session_state.data_processor = DataProcessor()
if 'vector_db' not in st.session_state:
    st.session_state.vector_db = VectorDatabase()
if 'llm_manager' not in st.session_state:
    st.session_state.llm_manager = LangChainLLMManager()
if 'processed_data' not in st.session_state:
    st.session_state.processed_data = None
if 'vector_index_built' not in st.session_state:
    st.session_state.vector_index_built = False
if 'modeling_results' not in st.session_state:
    st.session_state.modeling_results = None

def main():
    st.title("ğŸ§¬ ç–«è‹—ä¸»é¡Œå»ºæ¨¡RAGç³»çµ±")
    st.markdown("---")
    
    # å´é‚Šæ¬„é…ç½®
    with st.sidebar:
        st.header("âš™ï¸ ç³»çµ±é…ç½®")
        
        # LLMé…ç½®
        st.subheader("ğŸ¤– LLMé…ç½®")
        
        # ç²å–æ”¯æŒçš„æä¾›è€…
        supported_providers = st.session_state.llm_manager.get_supported_providers()
        
        provider = st.selectbox(
            "é¸æ“‡LLMæä¾›è€…",
            options=list(supported_providers.keys()),
            format_func=lambda x: x.upper()
        )
        
        model = st.selectbox(
            "é¸æ“‡æ¨¡å‹",
            options=supported_providers[provider]
        )
        
        api_key = st.text_input(
            f"è¼¸å…¥{provider.upper()} API Key",
            type="password",
            help=f"è«‹è¼¸å…¥æœ‰æ•ˆçš„{provider.upper()} APIå¯†é‘°"
        )
        
        # LLMåƒæ•¸é…ç½®
        st.subheader("ğŸ›ï¸ æ¨¡å‹åƒæ•¸")
        temperature = st.slider("Temperature", 0.0, 2.0, 0.7, 0.1)
        max_tokens = st.slider("Max Tokens", 100, 2000, 1000, 100)
        
        # æ¸¬è©¦é€£æ¥æŒ‰éˆ•
        if st.button("ğŸ”— æ¸¬è©¦LLMé€£æ¥"):
            if api_key:
                with st.spinner("æ¸¬è©¦é€£æ¥ä¸­..."):
                    success = st.session_state.llm_manager.test_connection(provider, model, api_key)
                    if success:
                        st.success("âœ… é€£æ¥æˆåŠŸï¼")
                        # å‰µå»ºä¸¦è¨­ç½®LLM
                        llm = st.session_state.llm_manager.create_llm(
                            provider, model, api_key,
                            temperature=temperature,
                            max_tokens=max_tokens
                        )
                        # å‰µå»ºé…ç½®å°è±¡
                        config = LLMConfig(
                            provider=LLMProvider(provider),
                            model=model,
                            api_key=api_key,
                            temperature=temperature,
                            max_tokens=max_tokens
                        )
                        st.session_state.llm_manager.set_active_llm(llm, config)
                        st.session_state.llm_configured = True
                    else:
                        st.error("âŒ é€£æ¥å¤±æ•—ï¼Œè«‹æª¢æŸ¥APIå¯†é‘°")
            else:
                st.warning("âš ï¸ è«‹å…ˆè¼¸å…¥APIå¯†é‘°")
        
        st.markdown("---")
        
        # æ•¸æ“šé…ç½®
        st.subheader("ğŸ“Š æ•¸æ“šé…ç½®")
        
        # æ•¸æ“šä¸Šå‚³
        uploaded_file = st.file_uploader(
            "ä¸Šå‚³CSVæ•¸æ“šæ–‡ä»¶",
            type=['csv'],
            help="è«‹ä¸Šå‚³åŒ…å«æ–‡æœ¬æ•¸æ“šçš„CSVæ–‡ä»¶"
        )
        
        if uploaded_file is not None:
            # ä¿å­˜ä¸Šå‚³çš„æ–‡ä»¶
            data_path = "data/uploaded_data.csv"
            os.makedirs("data", exist_ok=True)
            with open(data_path, "wb") as f:
                f.write(uploaded_file.getbuffer())
            st.success("âœ… æ–‡ä»¶ä¸Šå‚³æˆåŠŸ")
            
            # é è™•ç†æ•¸æ“š
            if st.button("ğŸ”„ é è™•ç†æ•¸æ“š"):
                with st.spinner("é è™•ç†æ•¸æ“šä¸­..."):
                    try:
                        data = st.session_state.data_processor.load_csv(data_path)
                        processed_data = st.session_state.data_processor.preprocess_data()
                        st.session_state.processed_data = processed_data
                        
                        # é¡¯ç¤ºçµ±è¨ˆä¿¡æ¯
                        stats = st.session_state.data_processor.get_text_stats(processed_data)
                        st.success("âœ… æ•¸æ“šé è™•ç†å®Œæˆ")
                        st.json(stats)
                        
                    except Exception as e:
                        st.error(f"âŒ é è™•ç†å¤±æ•—ï¼š{e}")
        
        # å»ºç«‹å‘é‡ç´¢å¼•
        if st.session_state.processed_data is not None and not st.session_state.vector_index_built:
            if st.button("ğŸ” å»ºç«‹å‘é‡ç´¢å¼•"):
                with st.spinner("å»ºç«‹å‘é‡ç´¢å¼•ä¸­..."):
                    try:
                        texts = st.session_state.processed_data['processed_text'].tolist()
                        st.session_state.vector_db.build_index(texts, save_path="data/vector_index")
                        st.session_state.vector_index_built = True
                        st.success("âœ… å‘é‡ç´¢å¼•å»ºç«‹æˆåŠŸ")
                    except Exception as e:
                        st.error(f"âŒ ç´¢å¼•å»ºç«‹å¤±æ•—ï¼š{e}")
    
    # ä¸»å…§å®¹å€åŸŸ
    if st.session_state.processed_data is not None:
        # é¡¯ç¤ºæ•¸æ“šæ¦‚è¦½
        st.header("ğŸ“Š æ•¸æ“šæ¦‚è¦½")
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("æ–‡æª”ç¸½æ•¸", len(st.session_state.processed_data))
        
        with col2:
            avg_words = st.session_state.processed_data['processed_text'].str.split().str.len().mean()
            st.metric("å¹³å‡è©æ•¸", f"{avg_words:.1f}")
        
        with col3:
            total_words = st.session_state.processed_data['processed_text'].str.split().str.len().sum()
            st.metric("ç¸½è©æ•¸", total_words)
        
        with col4:
            unique_words = len(set(' '.join(st.session_state.processed_data['processed_text']).split()))
            st.metric("ç¨ç‰¹è©å½™", unique_words)
        
        # é¡¯ç¤ºæ•¸æ“šæ¨£æœ¬
        st.subheader("ğŸ“ æ•¸æ“šæ¨£æœ¬")
        sample_data = st.session_state.processed_data.head(10)
        st.dataframe(sample_data[['processed_text']], use_container_width=True)
        
        # è©æ•¸åˆ†ä½ˆåœ–
        st.subheader("ğŸ“ˆ è©æ•¸åˆ†ä½ˆ")
        word_counts = st.session_state.processed_data['processed_text'].str.split().str.len()
        
        fig = px.histogram(
            x=word_counts,
            nbins=30,
            title="æ–‡æª”è©æ•¸åˆ†ä½ˆ",
            labels={'x': 'è©æ•¸', 'y': 'æ–‡æª”æ•¸é‡'}
        )
        fig.update_layout(showlegend=False)
        st.plotly_chart(fig, use_container_width=True)
        
        # ä¸»é¡Œå»ºæ¨¡åŸ·è¡Œå€åŸŸ
        if (st.session_state.vector_index_built and 
            hasattr(st.session_state, 'llm_configured') and 
            st.session_state.llm_configured):
            
            st.header("ğŸ¯ ä¸»é¡Œå»ºæ¨¡åŸ·è¡Œ")
            
            # åƒæ•¸é…ç½®
            with st.expander("âš™ï¸ å»ºæ¨¡åƒæ•¸è¨­ç½®", expanded=False):
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    num_topics = st.slider("ä¸»é¡Œæ•¸é‡", 5, 20, 10)
                    max_iterations = st.slider("æœ€å¤§è¿­ä»£æ¬¡æ•¸", 1, 10, 5)
                
                with col2:
                    quality_threshold = st.slider("è³ªé‡é–¾å€¼", 5.0, 10.0, 7.0, 0.1)
                    num_rounds = st.slider("è©•ä¼°è¼ªæ¬¡", 1, 10, 5)
                
                with col3:
                    query = st.text_input("æª¢ç´¢æŸ¥è©¢", "vaccine effectiveness and safety")
                    domain = st.text_input("é ˜åŸŸæè¿°", "vaccine research")
            
            col1, col2 = st.columns([2, 1])
            
            with col1:
                st.info("âœ… ç³»çµ±å·²æº–å‚™å°±ç·’ï¼Œå¯ä»¥é–‹å§‹ä¸»é¡Œå»ºæ¨¡")
            
            with col2:
                start_modeling = st.button("â–¶ï¸ é–‹å§‹å»ºæ¨¡", type="primary")
            
            if start_modeling:
                run_topic_modeling(
                    num_topics=num_topics,
                    max_iterations=max_iterations,
                    quality_threshold=quality_threshold,
                    num_rounds=num_rounds,
                    query=query,
                    domain=domain
                )
        
        else:
            st.header("âš ï¸ ç³»çµ±ç‹€æ…‹")
            
            status_items = [
                ("æ•¸æ“šé è™•ç†", st.session_state.processed_data is not None),
                ("å‘é‡ç´¢å¼•", st.session_state.vector_index_built),
                ("LLMé…ç½®", hasattr(st.session_state, 'llm_configured') and st.session_state.llm_configured)
            ]
            
            for item, status in status_items:
                icon = "âœ…" if status else "âŒ"
                st.write(f"{icon} {item}")
    
    else:
        # æ­¡è¿é é¢
        st.header("ğŸ‘‹ æ­¡è¿ä½¿ç”¨ç–«è‹—ä¸»é¡Œå»ºæ¨¡RAGç³»çµ±")
        
        st.markdown("""
        ### ğŸš€ é–‹å§‹ä½¿ç”¨
        
        1. **é…ç½®LLM**: åœ¨å·¦å´é‚Šæ¬„é¸æ“‡LLMæä¾›è€…ä¸¦è¼¸å…¥APIå¯†é‘°
        2. **ä¸Šå‚³æ•¸æ“š**: ä¸Šå‚³åŒ…å«ç–«è‹—ç›¸é—œæ–‡æœ¬çš„CSVæ–‡ä»¶
        3. **é è™•ç†æ•¸æ“š**: æ¸…ç†å’Œæ¨™æº–åŒ–æ–‡æœ¬æ•¸æ“š
        4. **å»ºç«‹ç´¢å¼•**: å‰µå»ºå‘é‡æ•¸æ“šåº«ç´¢å¼•
        5. **é–‹å§‹å»ºæ¨¡**: åŸ·è¡Œä¸»é¡Œå»ºæ¨¡å’Œè©•ä¼°
        
        ### ğŸ“‹ ç³»çµ±åŠŸèƒ½
        
        - **å¤šLLMæ”¯æŒ**: OpenAIã€Geminiã€Claude
        - **æ™ºèƒ½æª¢ç´¢**: åŸºæ–¼å‘é‡ç›¸ä¼¼åº¦çš„RAGæª¢ç´¢
        - **DSPy Agents**: æª¢ç´¢ã€æ€è€ƒã€è©•ä¼°ä¸‰é‡ä»£ç†
        - **å¤šç¶­è©•ä¼°**: æœ‰æ•ˆæ€§ã€å¤šæ¨£æ€§ã€å¯é æ€§è©•ä¼°
        - **è¦–è¦ºåŒ–å±•ç¤º**: äº’å‹•å¼çµæœå±•ç¤ºå’Œåˆ†æ
        
        ### ğŸ”§ æŠ€è¡“ç‰¹é»
        
        - **å‘é‡åŒ–**: Sentence-BERTåµŒå…¥
        - **å‘é‡æ•¸æ“šåº«**: FAISSé«˜æ•ˆæœç´¢
        - **è¿­ä»£å„ªåŒ–**: è‡ªå‹•è³ªé‡è©•ä¼°å’Œæ”¹é€²
        - **æ‰¹é‡è™•ç†**: æ”¯æŒå¤§è¦æ¨¡æ•¸æ“šé›†
        """)
    
    # é¡¯ç¤ºå»ºæ¨¡çµæœ
    if st.session_state.modeling_results:
        st.markdown("---")
        display_topic_modeling_results(st.session_state.modeling_results)

def run_topic_modeling(num_topics: int = 10,
                      max_iterations: int = 5,
                      quality_threshold: float = 7.0,
                      num_rounds: int = 5,
                      query: str = "vaccine effectiveness and safety",
                      domain: str = "vaccine research"):
    """åŸ·è¡Œä¸»é¡Œå»ºæ¨¡"""
    
    try:
        with st.spinner("ğŸ”„ æ­£åœ¨åŸ·è¡Œä¸»é¡Œå»ºæ¨¡..."):
            
            # å‰µå»ºé€²åº¦æ¢
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            # åˆå§‹åŒ–æ§åˆ¶å™¨
            status_text.text("æ­£åœ¨åˆå§‹åŒ–æ§åˆ¶å™¨...")
            progress_bar.progress(10)
            
            controller = LangChainTopicModelingController(
                vector_database=st.session_state.vector_db,
                llm_manager=st.session_state.llm_manager,
                max_iterations=max_iterations,
                quality_threshold=quality_threshold
            )
            
            # å‰µå»ºæ‰¹é‡å»ºæ¨¡å™¨
            status_text.text("æ­£åœ¨æº–å‚™æ‰¹é‡å»ºæ¨¡...")
            progress_bar.progress(20)
            
            batch_modeling = LangChainBatchTopicModeling(controller)
            
            # åŸ·è¡Œå¤šè¼ªå»ºæ¨¡
            status_text.text(f"æ­£åœ¨åŸ·è¡Œ {num_rounds} è¼ªä¸»é¡Œå»ºæ¨¡...")
            progress_bar.progress(30)
            
            batch_results = batch_modeling.run_multiple_rounds(
                num_rounds=num_rounds,
                query=query,
                domain=domain
            )
            
            progress_bar.progress(70)
            
            if batch_results.get('success', False):
                # åŸ·è¡Œè©³ç´°è©•ä¼°
                status_text.text("æ­£åœ¨åŸ·è¡Œè©³ç´°è©•ä¼°...")
                
                # å‰µå»ºè©•ä¼°å™¨
                effectiveness_evaluator = EffectivenessEvaluator(st.session_state.vector_db)
                diversity_evaluator = DiversityEvaluator()
                reliability_evaluator = ReliabilityEvaluator(
                    effectiveness_evaluator, diversity_evaluator, num_rounds
                )
                
                # è©•ä¼°æœ€ä½³çµæœ
                best_topics = batch_results.get('best_topics', [])
                
                if best_topics:
                    # æœ‰æ•ˆæ€§è©•ä¼°
                    effectiveness_metrics = effectiveness_evaluator.evaluate_topic_effectiveness(best_topics)
                    effectiveness_report = effectiveness_evaluator.generate_effectiveness_report(effectiveness_metrics, best_topics)
                    
                    # å¤šæ¨£æ€§è©•ä¼°
                    diversity_metrics = diversity_evaluator.evaluate_topic_diversity(best_topics)
                    diversity_report = diversity_evaluator.generate_diversity_report(diversity_metrics, best_topics)
                    
                    # å¯é æ€§è©•ä¼°
                    reliability_metrics = reliability_evaluator.evaluate_reliability(batch_results)
                    reliability_report = reliability_evaluator.generate_reliability_report(reliability_metrics)
                    
                    # æ•´åˆçµæœ
                    final_results = {
                        'success': True,
                        'topics': best_topics,
                        'topic_descriptions': batch_results.get('best_topic_descriptions', best_topics),
                        'confidence_scores': [0.8] * len(best_topics),  # æ¨¡æ“¬ä¿¡å¿ƒåˆ†æ•¸
                        'final_score': batch_results.get('best_score', 0),
                        'iterations_completed': batch_results.get('num_successful_rounds', 0),
                        'stop_reason': 'å¤šè¼ªè©•ä¼°å®Œæˆ',
                        'improvement_history': [],
                        'effectiveness_report': effectiveness_report,
                        'diversity_report': diversity_report,
                        'reliability_report': reliability_report,
                        'batch_results': batch_results
                    }
                    
                    progress_bar.progress(100)
                    status_text.text("âœ… ä¸»é¡Œå»ºæ¨¡å®Œæˆï¼")
                    
                    # ä¿å­˜çµæœ
                    st.session_state.modeling_results = final_results
                    
                    # é¡¯ç¤ºæˆåŠŸæ¶ˆæ¯
                    st.success(f"ğŸ‰ æˆåŠŸç”Ÿæˆ {len(best_topics)} å€‹ä¸»é¡Œï¼")
                    
                    # è‡ªå‹•åˆ·æ–°é é¢ä»¥é¡¯ç¤ºçµæœ
                    st.rerun()
                
                else:
                    st.error("âŒ æœªèƒ½ç”Ÿæˆæœ‰æ•ˆä¸»é¡Œ")
            
            else:
                error_msg = batch_results.get('error', 'æœªçŸ¥éŒ¯èª¤')
                st.error(f"âŒ ä¸»é¡Œå»ºæ¨¡å¤±æ•—: {error_msg}")
    
    except Exception as e:
        st.error(f"âŒ åŸ·è¡Œéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        logger.error(f"ä¸»é¡Œå»ºæ¨¡åŸ·è¡ŒéŒ¯èª¤: {e}")

if __name__ == "__main__":
    main()