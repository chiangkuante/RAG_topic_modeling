import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.figure_factory as ff
from typing import Dict, List, Any, Optional
import json
from datetime import datetime

def display_topic_modeling_results(results: Dict[str, Any]) -> None:
    """é¡¯ç¤ºä¸»é¡Œå»ºæ¨¡çµæœçš„ä¸»è¦ç•Œé¢"""
    
    if not results.get('success', False):
        st.error(f"âŒ ä¸»é¡Œå»ºæ¨¡å¤±æ•—: {results.get('error', 'æœªçŸ¥éŒ¯èª¤')}")
        return
    
    # çµæœæ¦‚è¦½
    display_results_overview(results)
    
    # è©³ç´°çµæœå±•ç¤º
    tab1, tab2, tab3, tab4 = st.tabs([
        "ğŸ¯ ä¸»é¡Œçµæœ", 
        "ğŸ“Š æœ‰æ•ˆæ€§åˆ†æ", 
        "ğŸŒˆ å¤šæ¨£æ€§åˆ†æ", 
        "ğŸ”„ å¯é æ€§åˆ†æ"
    ])
    
    with tab1:
        display_topics_section(results)
    
    with tab2:
        display_effectiveness_section(results)
    
    with tab3:
        display_diversity_section(results)
    
    with tab4:
        display_reliability_section(results)

def display_results_overview(results: Dict[str, Any]) -> None:
    """é¡¯ç¤ºçµæœæ¦‚è¦½"""
    
    st.header("ğŸ“‹ çµæœæ¦‚è¦½")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric(
            "ç”Ÿæˆä¸»é¡Œæ•¸", 
            len(results.get('topics', [])),
            help="æˆåŠŸç”Ÿæˆçš„ä¸»é¡Œç¸½æ•¸"
        )
    
    with col2:
        final_score = results.get('final_score', 0)
        st.metric(
            "æœ€çµ‚è©•åˆ†", 
            f"{final_score:.2f}",
            help="ä¸»é¡Œè³ªé‡çš„ç¶œåˆè©•åˆ† (0-10åˆ†)"
        )
    
    with col3:
        iterations = results.get('iterations_completed', 0)
        st.metric(
            "è¿­ä»£æ¬¡æ•¸", 
            iterations,
            help="å®Œæˆçš„å„ªåŒ–è¿­ä»£æ¬¡æ•¸"
        )
    
    with col4:
        stop_reason = results.get('stop_reason', 'æœªçŸ¥')
        st.metric(
            "åœæ­¢åŸå› ", 
            stop_reason,
            help="è¿­ä»£åœæ­¢çš„åŸå› "
        )
    
    # æ”¹é€²æ­·å²åœ–è¡¨
    if 'improvement_history' in results and results['improvement_history']:
        st.subheader("ğŸ“ˆ è¿­ä»£æ”¹é€²æ­·å²")
        
        improvement_history = results['improvement_history']
        iterations_list = list(range(1, len(improvement_history) + 1))
        
        fig = px.line(
            x=iterations_list,
            y=improvement_history,
            title="æ¯è¼ªè¿­ä»£çš„æ”¹é€²å¹…åº¦",
            labels={'x': 'è¿­ä»£è¼ªæ¬¡', 'y': 'æ”¹é€²å¹…åº¦'}
        )
        fig.update_layout(showlegend=False)
        st.plotly_chart(fig, use_container_width=True)

def display_topics_section(results: Dict[str, Any]) -> None:
    """é¡¯ç¤ºä¸»é¡Œçµæœéƒ¨åˆ†"""
    
    st.header("ğŸ¯ ç”Ÿæˆçš„ä¸»é¡Œ")
    
    topics = results.get('topics', [])
    topic_descriptions = results.get('topic_descriptions', [])
    confidence_scores = results.get('confidence_scores', [])
    
    if not topics:
        st.warning("âš ï¸ æ²’æœ‰æ‰¾åˆ°ç”Ÿæˆçš„ä¸»é¡Œ")
        return
    
    # ä¸»é¡Œåˆ—è¡¨å±•ç¤º
    st.subheader("ğŸ“ ä¸»é¡Œåˆ—è¡¨")
    
    # å‰µå»ºä¸»é¡Œæ•¸æ“šæ¡†
    topic_data = []
    for i, topic in enumerate(topics):
        description = topic_descriptions[i] if i < len(topic_descriptions) else topic
        confidence = confidence_scores[i] if i < len(confidence_scores) else 0.8
        
        topic_data.append({
            'åºè™Ÿ': i + 1,
            'ä¸»é¡Œ': topic,
            'æè¿°': description,
            'ä¿¡å¿ƒåˆ†æ•¸': f"{confidence:.3f}",
            'è©•ç´š': get_confidence_grade(confidence)
        })
    
    df_topics = pd.DataFrame(topic_data)
    st.dataframe(df_topics, use_container_width=True)
    
    # ä¿¡å¿ƒåˆ†æ•¸åˆ†ä½ˆ
    if confidence_scores:
        st.subheader("ğŸ“Š ä¸»é¡Œä¿¡å¿ƒåˆ†æ•¸åˆ†ä½ˆ")
        
        fig = px.bar(
            x=[f"ä¸»é¡Œ {i+1}" for i in range(len(confidence_scores))],
            y=confidence_scores,
            title="å„ä¸»é¡Œçš„ä¿¡å¿ƒåˆ†æ•¸",
            labels={'x': 'ä¸»é¡Œ', 'y': 'ä¿¡å¿ƒåˆ†æ•¸'},
            color=confidence_scores,
            color_continuous_scale='Viridis'
        )
        fig.update_layout(showlegend=False)
        st.plotly_chart(fig, use_container_width=True)
    
    # å¯ä¸‹è¼‰çš„çµæœ
    st.subheader("ğŸ’¾ å°å‡ºçµæœ")
    
    col1, col2 = st.columns(2)
    
    with col1:
        # JSONæ ¼å¼ä¸‹è¼‰
        json_data = json.dumps(results, indent=2, ensure_ascii=False, default=str)
        st.download_button(
            label="ğŸ“„ ä¸‹è¼‰JSONæ ¼å¼",
            data=json_data,
            file_name=f"topic_modeling_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
            mime="application/json"
        )
    
    with col2:
        # CSVæ ¼å¼ä¸‹è¼‰
        csv_data = df_topics.to_csv(index=False, encoding='utf-8')
        st.download_button(
            label="ğŸ“Š ä¸‹è¼‰CSVæ ¼å¼",
            data=csv_data,
            file_name=f"topics_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
            mime="text/csv"
        )

def display_effectiveness_section(results: Dict[str, Any]) -> None:
    """é¡¯ç¤ºæœ‰æ•ˆæ€§åˆ†æéƒ¨åˆ†"""
    
    st.header("ğŸ“Š æœ‰æ•ˆæ€§åˆ†æ")
    
    # æ¨¡æ“¬æœ‰æ•ˆæ€§æ•¸æ“šï¼ˆå¯¦éš›ä½¿ç”¨ä¸­æ‡‰è©²å¾è©•ä¼°çµæœç²å–ï¼‰
    effectiveness_data = generate_mock_effectiveness_data(results)
    
    # æœ‰æ•ˆæ€§æ¦‚è¦½
    st.subheader("ğŸ¯ æœ‰æ•ˆæ€§æ¦‚è¦½")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        avg_similarity = effectiveness_data.get('mean_similarity', 0)
        st.metric(
            "å¹³å‡ç›¸ä¼¼åº¦",
            f"{avg_similarity:.3f}",
            help="ä¸»é¡Œèˆ‡æ–‡æª”çš„å¹³å‡é¤˜å¼¦ç›¸ä¼¼åº¦"
        )
    
    with col2:
        coverage_score = effectiveness_data.get('coverage_score', 0)
        st.metric(
            "è¦†è“‹åˆ†æ•¸",
            f"{coverage_score:.3f}", 
            help="ä¸»é¡Œå°æ–‡æª”é›†åˆçš„è¦†è“‹ç¨‹åº¦"
        )
    
    with col3:
        effectiveness_grade = get_effectiveness_grade(avg_similarity)
        st.metric(
            "æœ‰æ•ˆæ€§ç­‰ç´š",
            effectiveness_grade,
            help="åŸºæ–¼ç›¸ä¼¼åº¦çš„æœ‰æ•ˆæ€§è©•ç´š"
        )
    
    # ç›¸ä¼¼åº¦åˆ†ä½ˆåœ–
    st.subheader("ğŸ“ˆ ç›¸ä¼¼åº¦åˆ†ä½ˆ")
    
    similarity_scores = effectiveness_data.get('similarity_scores', [])
    if similarity_scores:
        fig = px.histogram(
            x=similarity_scores,
            nbins=20,
            title="ä¸»é¡Œ-æ–‡æª”ç›¸ä¼¼åº¦åˆ†ä½ˆ",
            labels={'x': 'ç›¸ä¼¼åº¦', 'y': 'é »ç‡'}
        )
        st.plotly_chart(fig, use_container_width=True)
    
    # ä¸»é¡Œæ’å
    st.subheader("ğŸ† ä¸»é¡Œæœ‰æ•ˆæ€§æ’å")
    
    topic_rankings = effectiveness_data.get('topic_rankings', [])
    if topic_rankings:
        df_rankings = pd.DataFrame(topic_rankings)
        st.dataframe(df_rankings, use_container_width=True)

def display_diversity_section(results: Dict[str, Any]) -> None:
    """é¡¯ç¤ºå¤šæ¨£æ€§åˆ†æéƒ¨åˆ†"""
    
    st.header("ğŸŒˆ å¤šæ¨£æ€§åˆ†æ")
    
    # æ¨¡æ“¬å¤šæ¨£æ€§æ•¸æ“š
    diversity_data = generate_mock_diversity_data(results)
    
    # å¤šæ¨£æ€§æ¦‚è¦½
    st.subheader("ğŸ¨ å¤šæ¨£æ€§æ¦‚è¦½")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        diversity_score = diversity_data.get('diversity_score', 0)
        st.metric(
            "å¤šæ¨£æ€§åˆ†æ•¸",
            f"{diversity_score:.3f}",
            help="ä¸»é¡Œé–“å¤šæ¨£æ€§çš„ç¶œåˆè©•åˆ†"
        )
    
    with col2:
        unique_words_ratio = diversity_data.get('unique_words_ratio', 0)
        st.metric(
            "ç¨ç‰¹è©å½™æ¯”ä¾‹",
            f"{unique_words_ratio:.3f}",
            help="ç¨ç‰¹è©å½™ä½”ç¸½è©å½™çš„æ¯”ä¾‹"
        )
    
    with col3:
        lexical_diversity = diversity_data.get('lexical_diversity', 0)
        st.metric(
            "è©å½™å¤šæ¨£æ€§",
            f"{lexical_diversity:.3f}",
            help="è©å½™å±¤é¢çš„å¤šæ¨£æ€§è©•åˆ†"
        )
    
    with col4:
        semantic_diversity = diversity_data.get('semantic_diversity', 0)
        st.metric(
            "èªç¾©å¤šæ¨£æ€§",
            f"{semantic_diversity:.3f}",
            help="èªç¾©å±¤é¢çš„å¤šæ¨£æ€§è©•åˆ†"
        )
    
    # ä¸»é¡Œç›¸ä¼¼åº¦çŸ©é™£ç†±åœ–
    st.subheader("ğŸ”¥ ä¸»é¡Œç›¸ä¼¼åº¦çŸ©é™£")
    
    topics = results.get('topics', [])
    if len(topics) > 1:
        # ç”Ÿæˆæ¨¡æ“¬ç›¸ä¼¼åº¦çŸ©é™£
        similarity_matrix = generate_mock_similarity_matrix(len(topics))
        
        fig = px.imshow(
            similarity_matrix,
            x=[f"ä¸»é¡Œ{i+1}" for i in range(len(topics))],
            y=[f"ä¸»é¡Œ{i+1}" for i in range(len(topics))],
            title="ä¸»é¡Œé–“èªç¾©ç›¸ä¼¼åº¦çŸ©é™£",
            color_continuous_scale='RdYlBu_r'
        )
        st.plotly_chart(fig, use_container_width=True)
    
    # è©å½™é‡ç–Šåˆ†æ
    st.subheader("ğŸ“Š è©å½™é‡ç–Šåˆ†æ")
    
    word_analysis = diversity_data.get('word_analysis', {})
    if word_analysis:
        
        col1, col2 = st.columns(2)
        
        with col1:
            # è©å½™åˆ†ä½ˆé¤…åœ–
            labels = ['ç¨ç‰¹è©å½™', 'é‡è¤‡è©å½™']
            values = [
                word_analysis.get('unique_keywords', 0),
                word_analysis.get('repeated_keywords', 0)
            ]
            
            fig = px.pie(
                values=values,
                names=labels,
                title="è©å½™ç¨ç‰¹æ€§åˆ†ä½ˆ"
            )
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # æœ€å¸¸è¦‹é—œéµè©
            common_keywords = word_analysis.get('most_common_keywords', [])
            if common_keywords:
                df_keywords = pd.DataFrame(
                    common_keywords[:10], 
                    columns=['é—œéµè©', 'å‡ºç¾æ¬¡æ•¸']
                )
                st.write("**æœ€å¸¸è¦‹é—œéµè© (Top 10)**")
                st.dataframe(df_keywords, use_container_width=True)

def display_reliability_section(results: Dict[str, Any]) -> None:
    """é¡¯ç¤ºå¯é æ€§åˆ†æéƒ¨åˆ†"""
    
    st.header("ğŸ”„ å¯é æ€§åˆ†æ")
    
    # æ¨¡æ“¬å¯é æ€§æ•¸æ“š
    reliability_data = generate_mock_reliability_data(results)
    
    # å¯é æ€§æ¦‚è¦½
    st.subheader("ğŸ¯ å¯é æ€§æ¦‚è¦½")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        reliability_score = reliability_data.get('reliability_score', 0)
        st.metric(
            "å¯é æ€§åˆ†æ•¸",
            f"{reliability_score:.3f}",
            help="ç³»çµ±çµæœå¯é æ€§çš„ç¶œåˆè©•åˆ†"
        )
    
    with col2:
        stability_score = reliability_data.get('stability_score', 0)
        st.metric(
            "ç©©å®šæ€§åˆ†æ•¸",
            f"{stability_score:.3f}",
            help="å¤šè¼ªé‹è¡Œçµæœçš„ç©©å®šæ€§"
        )
    
    with col3:
        consistency_score = reliability_data.get('consistency_score', 0)
        st.metric(
            "ä¸€è‡´æ€§åˆ†æ•¸",
            f"{consistency_score:.3f}",
            help="çµæœçš„ä¸€è‡´æ€§ç¨‹åº¦"
        )
    
    with col4:
        confidence_interval = reliability_data.get('confidence_interval', (0, 0))
        ci_width = confidence_interval[1] - confidence_interval[0]
        st.metric(
            "ç½®ä¿¡å€é–“å¯¬åº¦",
            f"{ci_width:.3f}",
            help="95%ç½®ä¿¡å€é–“çš„å¯¬åº¦"
        )
    
    # å¤šè¼ªçµæœæ¯”è¼ƒ
    st.subheader("ğŸ“ˆ å¤šè¼ªçµæœæ¯”è¼ƒ")
    
    multi_round_scores = reliability_data.get('multi_round_scores', [])
    if multi_round_scores:
        
        fig = go.Figure()
        
        # æ·»åŠ åˆ†æ•¸ç·š
        fig.add_trace(go.Scatter(
            x=list(range(1, len(multi_round_scores) + 1)),
            y=multi_round_scores,
            mode='lines+markers',
            name='è©•åˆ†',
            line=dict(color='blue', width=2),
            marker=dict(size=8)
        ))
        
        # æ·»åŠ å¹³å‡ç·š
        avg_score = np.mean(multi_round_scores)
        fig.add_hline(
            y=avg_score, 
            line_dash="dash", 
            line_color="red",
            annotation_text=f"å¹³å‡åˆ†: {avg_score:.2f}"
        )
        
        fig.update_layout(
            title="å„è¼ªæ¬¡è©•åˆ†è®ŠåŒ–",
            xaxis_title="è¼ªæ¬¡",
            yaxis_title="è©•åˆ†",
            showlegend=True
        )
        
        st.plotly_chart(fig, use_container_width=True)
    
    # ç½®ä¿¡å€é–“åœ–
    st.subheader("ğŸ“Š ç½®ä¿¡å€é–“åˆ†æ")
    
    if multi_round_scores:
        mean_score = np.mean(multi_round_scores)
        std_score = np.std(multi_round_scores)
        
        fig = go.Figure()
        
        fig.add_trace(go.Bar(
            x=['æœ€å°å€¼', 'ä¸‹å››åˆ†ä½', 'ä¸­ä½æ•¸', 'ä¸Šå››åˆ†ä½', 'æœ€å¤§å€¼'],
            y=[
                np.min(multi_round_scores),
                np.percentile(multi_round_scores, 25),
                np.median(multi_round_scores),
                np.percentile(multi_round_scores, 75),
                np.max(multi_round_scores)
            ],
            name='åˆ†æ•¸åˆ†ä½ˆ'
        ))
        
        fig.update_layout(
            title="è©•åˆ†çµ±è¨ˆåˆ†ä½ˆ",
            yaxis_title="è©•åˆ†"
        )
        
        st.plotly_chart(fig, use_container_width=True)

def get_confidence_grade(confidence: float) -> str:
    """ç²å–ä¿¡å¿ƒåˆ†æ•¸ç­‰ç´š"""
    if confidence >= 0.9:
        return "å„ªç§€"
    elif confidence >= 0.8:
        return "è‰¯å¥½"
    elif confidence >= 0.7:
        return "ä¸­ç­‰"
    elif confidence >= 0.6:
        return "ä¸€èˆ¬"
    else:
        return "è¼ƒå·®"

def get_effectiveness_grade(similarity: float) -> str:
    """ç²å–æœ‰æ•ˆæ€§ç­‰ç´š"""
    if similarity >= 0.8:
        return "å„ªç§€"
    elif similarity >= 0.7:
        return "è‰¯å¥½"
    elif similarity >= 0.6:
        return "ä¸­ç­‰"
    elif similarity >= 0.4:
        return "ä¸€èˆ¬"
    else:
        return "è¼ƒå·®"

def generate_mock_effectiveness_data(results: Dict[str, Any]) -> Dict[str, Any]:
    """ç”Ÿæˆæ¨¡æ“¬æœ‰æ•ˆæ€§æ•¸æ“šï¼ˆå¯¦éš›å¯¦ç¾ä¸­æ‡‰å¾è©•ä¼°å™¨ç²å–ï¼‰"""
    
    num_topics = len(results.get('topics', []))
    final_score = results.get('final_score', 5.0)
    
    # åŸºæ–¼æœ€çµ‚åˆ†æ•¸ç”Ÿæˆæ¨¡æ“¬æ•¸æ“š
    base_similarity = min(0.9, max(0.1, final_score / 10))
    
    return {
        'mean_similarity': base_similarity,
        'max_similarity': min(1.0, base_similarity + 0.1),
        'min_similarity': max(0.0, base_similarity - 0.1),
        'coverage_score': base_similarity * 0.9,
        'similarity_scores': np.random.normal(base_similarity, 0.1, 100).tolist(),
        'topic_rankings': [
            {
                'æ’å': i + 1,
                'ä¸»é¡Œ': results.get('topics', [])[i] if i < num_topics else f"ä¸»é¡Œ{i+1}",
                'æœ‰æ•ˆæ€§åˆ†æ•¸': base_similarity + np.random.normal(0, 0.05),
                'æœ€å¤§ç›¸ä¼¼åº¦': min(1.0, base_similarity + np.random.uniform(0.1, 0.2))
            }
            for i in range(min(num_topics, 10))
        ]
    }

def generate_mock_diversity_data(results: Dict[str, Any]) -> Dict[str, Any]:
    """ç”Ÿæˆæ¨¡æ“¬å¤šæ¨£æ€§æ•¸æ“š"""
    
    num_topics = len(results.get('topics', []))
    
    return {
        'diversity_score': np.random.uniform(0.6, 0.9),
        'unique_words_ratio': np.random.uniform(0.7, 0.95),
        'lexical_diversity': np.random.uniform(0.6, 0.8),
        'semantic_diversity': np.random.uniform(0.5, 0.8),
        'word_analysis': {
            'unique_keywords': np.random.randint(50, 100),
            'repeated_keywords': np.random.randint(10, 30),
            'most_common_keywords': [
                ('vaccine', 8), ('effectiveness', 6), ('safety', 5),
                ('immune', 4), ('protection', 4), ('response', 3),
                ('clinical', 3), ('trial', 2), ('antibody', 2), ('dose', 2)
            ]
        }
    }

def generate_mock_reliability_data(results: Dict[str, Any]) -> Dict[str, Any]:
    """ç”Ÿæˆæ¨¡æ“¬å¯é æ€§æ•¸æ“š"""
    
    final_score = results.get('final_score', 5.0)
    
    # ç”Ÿæˆ5è¼ªæ¨¡æ“¬åˆ†æ•¸
    multi_round_scores = [
        final_score + np.random.normal(0, 0.3)
        for _ in range(5)
    ]
    
    return {
        'reliability_score': np.random.uniform(0.7, 0.9),
        'stability_score': 1 - (np.std(multi_round_scores) / np.mean(multi_round_scores)),
        'consistency_score': np.random.uniform(0.6, 0.8),
        'confidence_interval': (
            np.mean(multi_round_scores) - 1.96 * np.std(multi_round_scores),
            np.mean(multi_round_scores) + 1.96 * np.std(multi_round_scores)
        ),
        'multi_round_scores': multi_round_scores
    }

def generate_mock_similarity_matrix(num_topics: int) -> np.ndarray:
    """ç”Ÿæˆæ¨¡æ“¬ç›¸ä¼¼åº¦çŸ©é™£"""
    
    # å‰µå»ºå°ç¨±çŸ©é™£
    matrix = np.random.uniform(0.1, 0.7, (num_topics, num_topics))
    matrix = (matrix + matrix.T) / 2  # ä½¿çŸ©é™£å°ç¨±
    np.fill_diagonal(matrix, 1.0)  # å°è§’ç·šç‚º1
    
    return matrix

def display_evaluation_summary(effectiveness_data: Dict, 
                             diversity_data: Dict, 
                             reliability_data: Dict) -> None:
    """é¡¯ç¤ºè©•ä¼°æ‘˜è¦"""
    
    st.header("ğŸ“ˆ è©•ä¼°æ‘˜è¦")
    
    # ç¶œåˆè©•ä¼°é›·é”åœ–
    categories = ['æœ‰æ•ˆæ€§', 'å¤šæ¨£æ€§', 'å¯é æ€§', 'ç©©å®šæ€§', 'ä¸€è‡´æ€§']
    values = [
        effectiveness_data.get('mean_similarity', 0) * 10,
        diversity_data.get('diversity_score', 0) * 10,
        reliability_data.get('reliability_score', 0) * 10,
        reliability_data.get('stability_score', 0) * 10,
        reliability_data.get('consistency_score', 0) * 10
    ]
    
    fig = go.Figure()
    
    fig.add_trace(go.Scatterpolar(
        r=values,
        theta=categories,
        fill='toself',
        name='ç³»çµ±è¡¨ç¾'
    ))
    
    fig.update_layout(
        polar=dict(
            radialaxis=dict(
                visible=True,
                range=[0, 10]
            )),
        showlegend=True,
        title="ä¸»é¡Œå»ºæ¨¡ç³»çµ±ç¶œåˆè©•ä¼°"
    )
    
    st.plotly_chart(fig, use_container_width=True)
    
    # æ”¹é€²å»ºè­°
    st.subheader("ğŸ’¡ æ”¹é€²å»ºè­°")
    
    recommendations = []
    
    if effectiveness_data.get('mean_similarity', 0) < 0.6:
        recommendations.append("â€¢ è€ƒæ…®èª¿æ•´æª¢ç´¢åƒæ•¸ä»¥æé«˜ä¸»é¡Œç›¸é—œæ€§")
    
    if diversity_data.get('diversity_score', 0) < 0.6:
        recommendations.append("â€¢ å¢åŠ ä¸»é¡Œç”Ÿæˆçš„å¤šæ¨£æ€§ç­–ç•¥")
    
    if reliability_data.get('reliability_score', 0) < 0.7:
        recommendations.append("â€¢ æé«˜ç³»çµ±åƒæ•¸ç©©å®šæ€§ä»¥æ”¹å–„å¯é æ€§")
    
    if not recommendations:
        recommendations.append("â€¢ ç³»çµ±è¡¨ç¾è‰¯å¥½ï¼Œå»ºè­°ä¿æŒç•¶å‰è¨­ç½®")
    
    for rec in recommendations:
        st.write(rec)